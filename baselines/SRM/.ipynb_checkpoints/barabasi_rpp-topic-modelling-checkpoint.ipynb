{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Modified by Bidisha           #\n",
    "#################################\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from srm.db_manager import connect_to_db, execute_sql\n",
    "import time\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def param_estimate(t_train, T):\n",
    "    \"\"\" For accurate calculation, use the last timestamp from t_train as T and the rest shall be passed as \n",
    "    commandline arguments to estimate parameters\n",
    "    \"\"\"\n",
    "    t_args = \"\\t\".join(str(x) for x in t_train[:-1])\n",
    "    #print command_arg\n",
    "    res = !./rpp  $T $t_args\n",
    "    #print res\n",
    "    l, mu, sigma = (float(x) for x in \"\".join(res).split('\\t'))\n",
    "    #print l, mu, sigma\n",
    "    return l, mu, sigma\n",
    "\n",
    "def predict(l, mu, sigma, T, nd, tp) :\n",
    "    m = 10 # Prior bilief. Make sure m_m in mic_model.cpp has the same value\n",
    "    f_tp = norm.cdf((math.log(tp) - mu)/sigma)\n",
    "    norm.cdf(f_tp)    \n",
    "    f_T = norm.cdf((math.log(T) - mu)/sigma)\n",
    "    norm.cdf(f_T)\n",
    "    cn = (m + nd)*math.exp((f_tp - f_T)*l) - m\n",
    "    #print \"Predicted\", int(math.ceil(cn))\n",
    "    #print cn\n",
    "    try:\n",
    "        te = int(math.ceil(cn))\n",
    "        return te\n",
    "    except:\n",
    "        #print cn\n",
    "        return -1\n",
    "    \n",
    "def mape(ta, tp):\n",
    "    \"\"\"Calculated MAPE for actual values ta and predicted values tp\n",
    "    \"\"\"\n",
    "    diff = 0\n",
    "    n = len(ta)\n",
    "    for i in range(n):\n",
    "        diff+= abs(ta[i]-tp[i])/float(ta[i])\n",
    "    return diff/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_name, table = \"srm\", \"BigBillionDay\"\n",
    "cursor_mysql, conn = connect_to_db(\"localhost\", \"root\", \"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of tweet that got retweeted and a reply from flipkart handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replied_tweets = ['518960274938667009','519169043714932736','519036348867428353']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = execute_sql(\"SELECT * FROM srm.BigBillionDayHashtags order by hashtag;\")\n",
    "hashtag = \"\"\n",
    "listOfHashTags = {}\n",
    "for row in results:\n",
    "    hashtag = row[1]\n",
    "    if(listOfHashTags.has)\n",
    "    listOfHashTags[hashtag].append(row[0])\n",
    "\n",
    "parent_tweets = [row[0] for row in execute_sql(\"select parent_id_str from srm.BigBillionDay where parent_id_str in \\\n",
    "(SELECT  distinct id_str from srm.BigBillionDay) \\\n",
    " group by parent_id_str having count(*) >50 order by count(*) desc;\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We consider the tweets that got replied in two sets\n",
    "1. Tweets that got a reply from flipkart\n",
    "2. Other tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tweet_set = list(set(parent_tweets)set(replied_tweets))\n",
    "tweet_set = replied_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "[1, 1, 1, 82, 240, 343, 386, 395, 433, 591, 670, 836, 1088, 1284, 1453, 1481, 1547, 1590, 1638, 1641, 1766, 1855, 1858, 2016, 2064, 2592, 3461, 3697, 5156, 5157, 5191, 5248, 5251, 5264, 5280, 5340, 5379, 5385, 5399]\n",
      "117.542 25.205 7.54242\n",
      "( 39 39 )\n",
      "( 40 40 )\n",
      "( 41 40 )\n",
      "( 42 41 )\n",
      "( 43 42 )\n",
      "( 44 46 )\n",
      "( 45 46 )\n",
      "( 46 46 )\n",
      "( 47 46 )\n",
      "( 48 47 )\n",
      "( 49 48 )\n",
      "( 50 56 )\n",
      "( 51 58 )\n",
      "( 52 102 )\n",
      "( 53 182 )\n",
      "( 54 193 )\n",
      "( 55 217 )\n"
     ]
    }
   ],
   "source": [
    "prediction_dict = defaultdict(list) #{tweetid:(time after training, real retweet#, predicted retweet#)}\n",
    "\n",
    "original = []\n",
    "for parent_id in tweet_set :\n",
    "    #print \"Parent ID\", parent_id\n",
    "    retweet_times = [row[0] for row in execute_sql(\"select created_at from srm.BigBillionDay where parent_id_str \\\n",
    "                                                            =%s order by created_at\", parent_id)]\n",
    "    parent_created_time = [row[0] for row in execute_sql(\"select created_at from srm.BigBillionDay where id_str \\\n",
    "                                                            =%s\", parent_id)]\n",
    "    created_times = parent_created_time #Stores time at which the tweet and retweets are created\n",
    "    created_times.extend(retweet_times)\n",
    "    t_original = list()\n",
    "    for ts in created_times : \n",
    "        t_original.append(int(time.mktime(ts.timetuple())-time.mktime(created_times[0].timetuple())))\n",
    "    \n",
    "    #To avoid errors in calculation, as suggested in the paper, we add a constant 1 to the time momemnts\n",
    "    t_original = [x+1 for x in t_original]\n",
    "    original = sorted(original + t_original)\n",
    "    \n",
    "reply_index = int(len(original)*.7)    \n",
    "t_train, t_test = original[:reply_index], original[reply_index:]\n",
    "      \n",
    "    #Training \n",
    "T = t_train[-1]\n",
    "l, mu, sigma = param_estimate(t_train, T)\n",
    "\n",
    "print \"Training\"\n",
    "print t_train\n",
    "print l, mu, sigma\n",
    "    \n",
    "    #Calculation of MAPE\n",
    "nd = len(t_train[:-1])\n",
    "parent_id = tweet_set[0]\n",
    "i = 0\n",
    "for tp in t_test:\n",
    "        te = predict(l, mu, sigma, T, nd, tp)\n",
    "        print \"(\", len(t_train)+i, te,\")\" \n",
    "        i = i + 1\n",
    "        if te == -1 : \n",
    "            continue\n",
    "        #print \"Real\", t_original.index(tp)+1\n",
    "        if not prediction_dict.has_key(parent_id):\n",
    "            prediction_dict[parent_id] = [(tp-T, original.index(tp)+1, te)]\n",
    "        else :\n",
    "            prediction_dict[parent_id].append((tp-T, original.index(tp)+1, te))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print MAPE for each tweet ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mape_dict = dict()\n",
    "for tweetid in prediction_dict.keys():\n",
    "    xp = [x[1] for x in prediction_dict[tweetid]]\n",
    "    xe = [x[2] for x in prediction_dict[tweetid]]\n",
    "    mape_val = mape(xp, xe)\n",
    "    mape_dict[tweetid] = (mape_val, len(xp))\n",
    "#print mape_dict[mape_dict.keys()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate cumulative MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cumulative MAPE 0.543962981674\n",
      "Number of outliers 0\n"
     ]
    }
   ],
   "source": [
    "outlier_count = 0\n",
    "weighted_sum = 0\n",
    "for tweetid in mape_dict.keys():\n",
    "    if mape_dict[tweetid][0] > 5 :\n",
    "        outlier_count+=1\n",
    "        print \"Outlier found id = %s, MAPE = %f\"%(tweetid, mape_dict[tweetid][0])\n",
    "        continue\n",
    "    weighted_sum+= mape_dict[tweetid][0]*mape_dict[tweetid][1]\n",
    "print \"\\n\\nCumulative MAPE\", float(weighted_sum)/sum([x[1] for x in mape_dict.values()])\n",
    "print \"Number of outliers\", outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = cm.rainbow(np.linspace(0, 1, 11))\n",
    "for tweetid,c in zip(prediction_dict.keys()[5:6], colors):\n",
    "    xp = [x[0] for x in prediction_dict[tweetid]]\n",
    "    xe = [ abs(x[1]-x[2])/float(x[1]) for x in prediction_dict[tweetid]]\n",
    "    plt.xlabel('Retweted time', fontsize=11)\n",
    "    plt.ylabel('Absolute Percentage Error', fontsize=11)\n",
    "    plt.title(\"id=\"+tweetid, fontsize=12)\n",
    "    plt.plot(xp, xe, color=c, marker='.')\n",
    "    print len(prediction_dict[tweetid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n"
     ]
    }
   ],
   "source": [
    "print type(parent_tweets[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
